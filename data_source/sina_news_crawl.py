"""
sina news data crawler
"""
import asyncio
import aiohttp
import re
import json
import os
import time
import hashlib
from datetime import datetime
import random
import html
import pandas as pd
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))
from .data_source_base import DataSourceBase
from models.llm_model import GLOBAL_LLM
from loguru import logger


class SinaNewsCrawl(DataSourceBase):
    def __init__(self, start_page=1, end_page=25):
        super().__init__("sina_news_crawl")
        self.start_page = start_page
        self.end_page = end_page
        # ä½¿ç”¨ä½ æä¾›çš„å®Œæ•´URLæ ¼å¼ï¼Œpage/r/callback å°†åœ¨è¯·æ±‚æ—¶åŠ¨æ€ç”Ÿæˆ
        self.base_url = "http://feed.mix.sina.com.cn/api/roll/get"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36",
            "Referer": "https://finance.sina.com.cn/",
            "Accept": "*/*",
            "Accept-Language": "zh-CN,zh;q=0.9",
        }
        self.all_items = []
        self.fetch_full_intro = True  # æ˜¯å¦æŠ“å–æ–‡ç« é¡µä»¥è¡¥å…¨ intro
        self.article_concurrency = 2 # æ§åˆ¶æŠ“å–æ–‡ç« é¡µçš„å¹¶å‘
        # LLMå¤„ç†ç¼“å­˜ç›®å½•
        self.llm_cache_dir = self.data_cache_dir / "llm_processed"
        self.llm_cache_dir.mkdir(parents=True, exist_ok=True)
        
    async def fetch_page(self, session, page):
        """å¼‚æ­¥è·å–å•ä¸ªé¡µé¢çš„æ•°æ®"""
        params = {
            "pageid": 384,
            "lid": 2519,
            "k": "",
            "num": 50,
            "page": page
        }
        
        try:
            async with session.get(self.base_url, params=params, headers=self.headers, timeout=15) as response:
                text = await response.text()
                
                # å…¼å®¹ JSONP ä¸ çº¯ JSON
                m = re.search(r'^\s*[\w$]+\((.*)\)\s*;?\s*$', text.strip(), re.S)
                json_text = m.group(1) if m else text.strip()
                data = json.loads(json_text)
                
                # æå–itemsï¼ˆä»…ä¿ç•™æŒ‡å®šå­—æ®µï¼‰
                items = self.extract_items(data, page)

                # å°è¯•è¡¥å…¨ intro
                if self.fetch_full_intro and items:
                    await self.enrich_items_with_full_intro(session, items)

                return items
                
        except Exception as e:
            return []
    
    def extract_items(self, data, page):
        """æå–æ–°é—»itemsï¼Œå¹¶è£å‰ªä¸ºç›®æ ‡å­—æ®µé›†"""
        try:
            if isinstance(data, dict):
                result = data.get("result", {})
                if isinstance(result, dict):
                    data_field = result.get("data", [])
                    if isinstance(data_field, list):
                        processed_items = []
                        for raw in data_field:
                            if not isinstance(raw, dict):
                                continue
                            # é€‰å–ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ—¶é—´å­—æ®µ
                            candidate_keys = [
                                "ctime", "intime", "mtime", "create_time", "createtime",
                                "pub_time", "pubTime", "pubdate", "pubDate", "time", "update_time"
                            ]
                            raw_time_value = None
                            for key in candidate_keys:
                                if key in raw and raw.get(key) not in (None, ""):
                                    raw_time_value = raw.get(key)
                                    break
                            publish_time = self.normalize_publish_time(raw_time_value)

                            # æœ¬åœ°å¯ç”¨çš„ç®€ä»‹
                            intro_local = self.choose_best_intro_local(raw)
                            # ç›®æ ‡URLï¼ˆä¼˜å…ˆPCï¼Œå…¶æ¬¡WAPï¼Œå…¶æ¬¡urlsæ•°ç»„ï¼‰
                            url = self.choose_best_url(raw)

                            # ä»…ä¿ç•™æŒ‡å®šå­—æ®µ
                            processed_items.append({
                                "title": raw.get("title") or raw.get("stitle") or "",
                                "intro": intro_local or "",
                                "publish_time": publish_time,
                                "media_name": raw.get("media_name") or "",
                                "url": url or "",
                            })
                        return processed_items
            return []
        except Exception as e:
            print(f"ç¬¬ {page} é¡µæ•°æ®è§£æå¤±è´¥: {e}")
            return []
    
    def normalize_publish_time(self, raw_value):
        """å°†å¤šç§æ—¶é—´æ ¼å¼æ ‡å‡†åŒ–ä¸º 'YYYY-MM-DD HH:MM:SS' å­—ç¬¦ä¸²"""
        try:
            if raw_value is None:
                return None
            # æ•°å­—æ—¶é—´æˆ³ï¼ˆç§’æˆ–æ¯«ç§’ï¼‰
            if isinstance(raw_value, (int, float)):
                timestamp = int(raw_value)
            elif isinstance(raw_value, str) and re.fullmatch(r"\d{10,13}", raw_value):
                timestamp = int(raw_value)
            else:
                # å°è¯•è§£æå¸¸è§çš„æ—¶é—´å­—ç¬¦ä¸²
                if isinstance(raw_value, str):
                    for fmt in [
                        "%Y-%m-%d %H:%M:%S",
                        "%Y-%m-%d %H:%M",
                        "%Y-%m-%d",
                        "%Y/%m/%d %H:%M:%S",
                        "%Y/%m/%d %H:%M",
                        "%Y/%m/%d",
                        "%Yå¹´%mæœˆ%dæ—¥ %H:%M",
                        "%Yå¹´%mæœˆ%dæ—¥",
                    ]:
                        try:
                            dt = datetime.strptime(raw_value.strip(), fmt)
                            return dt.strftime("%Y-%m-%d %H:%M:%S")
                        except Exception:
                            pass
                # æ— æ³•è¯†åˆ«åˆ™åŸæ ·è¿”å›å­—ç¬¦ä¸²
                return str(raw_value)

            # æ¯«ç§’ä¸ç§’çš„åŒºåˆ†
            if timestamp > 1_000_000_000_000:
                timestamp //= 1000
            elif 0 < timestamp < 10_000_000_000:
                pass
            else:
                # éå¸¸è§„èŒƒå›´ï¼Œä¿é™©èµ·è§å–å‰10ä½
                timestamp = int(str(timestamp)[:10])

            return datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")
        except Exception:
            return str(raw_value)

    def choose_best_url(self, raw_item):
        """é€‰æ‹©æœ€åˆé€‚çš„æ–‡ç« URL"""
        url = raw_item.get("url")
        if url:
            return url
        # æœ‰äº›è¿”å›çš„ urls æ˜¯ JSON å­—ç¬¦ä¸²
        urls_field = raw_item.get("urls")
        if isinstance(urls_field, list) and urls_field:
            return urls_field[0]
        if isinstance(urls_field, str) and urls_field.strip().startswith("["):
            try:
                parsed = json.loads(urls_field)
                if isinstance(parsed, list) and parsed:
                    return parsed[0]
            except Exception:
                pass
        wapurl = raw_item.get("wapurl")
        if wapurl:
            return wapurl
        return None

    def choose_best_intro_local(self, raw_item):
        """åœ¨ä¸è¯·æ±‚æ–‡ç« é¡µçš„æƒ…å†µä¸‹ï¼Œé€‰å–æœ€åˆé€‚çš„ç®€ä»‹å­—æ®µ"""
        candidates = [raw_item.get("intro"), raw_item.get("summary"), raw_item.get("wapsummary")]
        candidates = [c for c in candidates if isinstance(c, str) and c.strip()]
        if not candidates:
            return None
        # é€‰æ‹©æœ€é•¿çš„ä¸€æ¡
        best = max(candidates, key=lambda x: len(x))
        return best

    def should_fetch_full_intro(self, intro_text):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æŠ“å–æ–‡ç« é¡µè¡¥å…¨ç®€ä»‹"""
        if not intro_text:
            return True
        text = intro_text.strip()
        if len(text) < 60:
            return True
        if text.endswith("â€¦") or text.endswith("..."):
            return True
        return False

    async def enrich_items_with_full_intro(self, session, items):
        """å¹¶å‘æŠ“å–æ–‡ç« é¡µï¼Œè¡¥å…¨ intro"""
        semaphore = asyncio.Semaphore(self.article_concurrency)

        async def process_one(item):
            if not self.should_fetch_full_intro(item.get("intro")):
                return
            url = item.get("url")
            if not url:
                return
            try:
                async with semaphore:
                    intro_full = await self.fetch_article_intro(session, url)
                if intro_full and len(intro_full) > len(item.get("intro") or ""):
                    item["intro"] = intro_full
            except Exception:
                pass

        await asyncio.gather(*[process_one(it) for it in items])

    async def fetch_article_intro(self, session, url):
        """æŠ“å–æ–‡ç« é¡µç®€ä»‹ï¼šä¼˜å…ˆ meta description / og:descriptionï¼Œå…¶æ¬¡æ­£æ–‡é¦–æ®µ"""
        try:
            async with session.get(url, headers=self.headers, timeout=15) as resp:
                html_text = await resp.text(errors="ignore")
            if not html_text:
                return None
            # å…ˆå°è¯• meta description / og:description
            meta_desc = self._extract_meta_description(html_text)
            if meta_desc:
                return meta_desc
            # é€€åŒ–åˆ°æ­£æ–‡é¦–æ®µ
            first_paragraph = self._extract_first_paragraph(html_text)
            if first_paragraph:
                return first_paragraph
            return None
        except Exception:
            return None

    def _extract_meta_description(self, html_text):
        """ä»HTMLä¸­æå–<meta name="description">æˆ–<meta property="og:description">"""
        try:
            # name=description
            m1 = re.search(r'<meta[^>]+name=["\']description["\'][^>]+content=["\'](.*?)["\']', html_text, re.I | re.S)
            if m1:
                return html.unescape(self._clean_whitespace(m1.group(1)))
            # property=og:description
            m2 = re.search(r'<meta[^>]+property=["\']og:description["\'][^>]+content=["\'](.*?)["\']', html_text, re.I | re.S)
            if m2:
                return html.unescape(self._clean_whitespace(m2.group(1)))
            return None
        except Exception:
            return None

    def _extract_first_paragraph(self, html_text):
        """ä»å¸¸è§å®¹å™¨ä¸­æå–é¦–æ®µæ–‡æœ¬ï¼ˆç®€æ˜“æ­£åˆ™ç‰ˆï¼‰"""
        try:
            # å¸¸è§æ­£æ–‡å®¹å™¨ id/class: artibody, article, content
            container_patterns = [
                r'<div[^>]+id=["\']artibody["\'][^>]*>(.*?)</div>',
                r'<article[^>]*>(.*?)</article>',
                r'<div[^>]+class=["\'][^"\']*(?:article|content)[^"\']*["\'][^>]*>(.*?)</div>',
            ]
            for pat in container_patterns:
                m = re.search(pat, html_text, re.I | re.S)
                if m:
                    inner = m.group(1)
                    # æ‰¾ç¬¬ä¸€ä¸ª<p>
                    p = re.search(r'<p[^>]*>(.*?)</p>', inner, re.I | re.S)
                    if p:
                        text = self._strip_html_tags(p.group(1))
                        return self._clean_whitespace(text)
            # å…œåº•ï¼šå…¨å±€ç¬¬ä¸€ä¸ª<p>
            p = re.search(r'<p[^>]*>(.*?)</p>', html_text, re.I | re.S)
            if p:
                text = self._strip_html_tags(p.group(1))
                return self._clean_whitespace(text)
            return None
        except Exception:
            return None

    def _strip_html_tags(self, text):
        text = re.sub(r'<script[\s\S]*?</script>', ' ', text, flags=re.I)
        text = re.sub(r'<style[\s\S]*?</style>', ' ', text, flags=re.I)
        text = re.sub(r'<[^>]+>', ' ', text)
        return html.unescape(text)

    def _clean_whitespace(self, text):
        return re.sub(r'\s+', ' ', (text or '')).strip()
    
    def _get_llm_cache_key(self, df: pd.DataFrame) -> str:
        """ç”ŸæˆLLMç¼“å­˜çš„é”®"""
        # åŸºäºæ–°é—»å†…å®¹çš„å“ˆå¸Œå€¼ç”Ÿæˆç¼“å­˜é”®
        content_str = ""
        for _, row in df.iterrows():
            content_str += f"{row.get('title', '')}_{row.get('content', '')}_{row.get('url', '')}"
        return hashlib.md5(content_str.encode('utf-8')).hexdigest()
    
    def _save_llm_cache(self, cache_key: str, processed_df: pd.DataFrame) -> None:
        """ä¿å­˜LLMå¤„ç†ç»“æœåˆ°ç¼“å­˜"""
        try:
            cache_file = self.llm_cache_dir / f"{cache_key}.pkl"
            processed_df.to_pickle(cache_file)
            logger.info(f"LLMå¤„ç†ç»“æœå·²ç¼“å­˜: {cache_file}")
        except Exception as e:
            logger.warning(f"ä¿å­˜LLMç¼“å­˜å¤±è´¥: {e}")
    
    def _load_llm_cache(self, cache_key: str) -> pd.DataFrame:
        """ä»ç¼“å­˜åŠ è½½LLMå¤„ç†ç»“æœ"""
        try:
            cache_file = self.llm_cache_dir / f"{cache_key}.pkl"
            if cache_file.exists():
                # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
                cache_time = datetime.fromtimestamp(cache_file.stat().st_mtime)
                if (datetime.now() - cache_time).total_seconds() < 24 * 3600:
                    logger.info(f"ä»ç¼“å­˜åŠ è½½LLMå¤„ç†ç»“æœ: {cache_file}")
                    return pd.read_pickle(cache_file)
                else:
                    logger.info(f"LLMç¼“å­˜å·²è¿‡æœŸï¼Œåˆ é™¤: {cache_file}")
                    cache_file.unlink()
        except Exception as e:
            logger.warning(f"åŠ è½½LLMç¼“å­˜å¤±è´¥: {e}")
        return None
    
    async def process_news_with_llm(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä½¿ç”¨LLMæ•´ç†æ–°é—»å†…å®¹ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        try:
            if df.empty:
                return df
            
            # é™åˆ¶å¤„ç†æ•°é‡ï¼Œé¿å…è¿‡å¤šè¯·æ±‚
            max_news_count = 50
            df_to_process = df.head(max_news_count)
            
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = self._get_llm_cache_key(df_to_process)
            
            # å°è¯•ä»ç¼“å­˜åŠ è½½
            cached_result = self._load_llm_cache(cache_key)
            if cached_result is not None:
                logger.info(f"ä½¿ç”¨LLMç¼“å­˜ï¼Œè¿”å› {len(cached_result)} æ¡å·²å¤„ç†çš„æ–°é—»")
                return cached_result
            
            logger.info(f"å¼€å§‹ä½¿ç”¨LLMæ•´ç† {len(df_to_process)} æ¡æ–°é—»å†…å®¹ï¼ˆé™åˆ¶{max_news_count}æ¡ï¼‰")
            
            # åˆ†æ‰¹å¤„ç†æ–°é—»ï¼Œé¿å…å•æ¬¡è¯·æ±‚è¿‡å¤§
            batch_size = 10
            processed_rows = []
            
            for i in range(0, len(df_to_process), batch_size):
                batch_df = df_to_process.iloc[i:i+batch_size]
                batch_processed = await self._process_news_batch(batch_df)
                processed_rows.extend(batch_processed)
                
                # æ·»åŠ è¯·æ±‚é—´éš”ï¼Œé¿å…APIé™æµ
                if i + batch_size < len(df_to_process):
                    await asyncio.sleep(2)  # ç­‰å¾…2ç§’
            
            # åˆ›å»ºæ–°çš„DataFrame
            processed_df = pd.DataFrame(processed_rows)
            logger.info(f"LLMæ•´ç†å®Œæˆï¼Œä¿ç•™ {len(processed_df)} æ¡ç›¸å…³æ–°é—»")
            
            # ä¿å­˜åˆ°ç¼“å­˜
            self._save_llm_cache(cache_key, processed_df)
            
            return processed_df
            
        except Exception as e:
            logger.error(f"LLMå¤„ç†æ–°é—»å¤±è´¥: {e}")
            return df
    
    async def _process_news_batch(self, batch_df: pd.DataFrame) -> list:
        """å¤„ç†ä¸€æ‰¹æ–°é—»"""
        try:
            # æ„å»ºæ–°é—»æ–‡æœ¬
            news_text = self._build_news_text(batch_df)
            
            # è°ƒç”¨LLMå¤„ç†
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ–°é—»å†…å®¹ï¼Œç­›é€‰å‡ºä¸é‡‘èå¸‚åœºã€è‚¡ç¥¨ã€ç»æµã€æŠ•èµ„ç›¸å…³çš„æ–°é—»ï¼Œå¹¶æ•´ç†æˆè‡ªç„¶è¯­è¨€å½¢å¼ã€‚

åŸå§‹æ–°é—»å†…å®¹ï¼š
{news_text}

è¯·æŒ‰ä»¥ä¸‹è¦æ±‚å¤„ç†ï¼š
1. ç­›é€‰å‡ºä¸é‡‘èå¸‚åœºã€è‚¡ç¥¨ã€ç»æµã€æŠ•èµ„ã€æ”¿ç­–ç›¸å…³çš„æ–°é—»
2. åˆ é™¤ä¸é‡‘èæ— å…³çš„å¨±ä¹ã€ä½“è‚²ã€ç¤¾ä¼šæ–°é—»
3. å°†æ¯æ¡æ–°é—»æ•´ç†æˆç®€æ´ã€æ¸…æ™°çš„è‡ªç„¶è¯­è¨€æè¿°
4. ä¿æŒæ–°é—»çš„æ—¶æ•ˆæ€§å’Œé‡è¦æ€§
5. æ¯æ¡æ–°é—»æ§åˆ¶åœ¨100å­—ä»¥å†…

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆJSONæ ¼å¼ï¼‰ï¼š
{{
    "relevant_news": [
        {{
            "title": "æ–°é—»æ ‡é¢˜",
            "content": "æ•´ç†åçš„æ–°é—»å†…å®¹",
            "pub_time": "å‘å¸ƒæ—¶é—´",
            "url": "æ–°é—»é“¾æ¥",
            "importance": "high/medium/low"
        }}
    ]
}}

åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š
"""
            
            messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èæ–°é—»åˆ†æå¸ˆï¼Œæ“…é•¿ç­›é€‰å’Œæ•´ç†é‡‘èç›¸å…³çš„æ–°é—»å†…å®¹ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼è¾“å‡ºã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ]
            
            response = await asyncio.wait_for(
                GLOBAL_LLM.a_run(
                    messages=messages,
                    temperature=0.3,
                    max_tokens=2000
                ),
                timeout=60.0  # è®¾ç½®60ç§’è¶…æ—¶
            )
            
            if response and response.content:
                return self._parse_llm_response(response.content)
            else:
                logger.warning("LLMæœªè¿”å›æœ‰æ•ˆå“åº”ï¼Œè¿”å›åŸå§‹æ•°æ®")
                return batch_df.to_dict('records')
                
        except asyncio.TimeoutError:
            logger.warning(f"LLMå¤„ç†è¶…æ—¶ï¼Œè¿”å›åŸå§‹æ•°æ®")
            return batch_df.to_dict('records')
        except Exception as e:
            logger.error(f"å¤„ç†æ–°é—»æ‰¹æ¬¡å¤±è´¥: {e}")
            return batch_df.to_dict('records')
    
    def _build_news_text(self, df: pd.DataFrame) -> str:
        """æ„å»ºæ–°é—»æ–‡æœ¬"""
        news_items = []
        for _, row in df.iterrows():
            item = f"æ ‡é¢˜: {row.get('title', '')}\n"
            item += f"å†…å®¹: {row.get('content', '')}\n"
            item += f"æ—¶é—´: {row.get('pub_time', '')}\n"
            item += f"é“¾æ¥: {row.get('url', '')}\n"
            item += "---\n"
            news_items.append(item)
        return "\n".join(news_items)
    
    def _parse_llm_response(self, response_text: str) -> list:
        """è§£æLLMå“åº”"""
        try:
            import json
            
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                result = json.loads(json_str)
                
                if 'relevant_news' in result and isinstance(result['relevant_news'], list):
                    return result['relevant_news']
            
            logger.warning("æ— æ³•è§£æLLMå“åº”ä¸ºæœ‰æ•ˆJSON")
            return []
            
        except Exception as e:
            logger.error(f"è§£æLLMå“åº”å¤±è´¥: {e}")
            return []
    
    async def crawl_all_pages(self):
        start_time = time.time()
        
        timeout = aiohttp.ClientTimeout(total=30)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            tasks = []
            for page in range(self.start_page, self.end_page + 1):
                task = self.fetch_page(session, page)
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for page, result in enumerate(results, start=self.start_page):
                if isinstance(result, Exception):
                    print(f"ç¬¬ {page} é¡µå‘ç”Ÿå¼‚å¸¸: {result}")
                elif isinstance(result, list):
                    self.all_items.extend(result)
        return self.all_items
    

    async def get_data(self, trigger_time: str) -> pd.DataFrame:
        self.all_items = []  # æ¸…ç©ºç´¯ç§¯çš„æ•°æ®
        
        try:
            items = await self.crawl_all_pages()
        except Exception as e:
            logger.error(f"âŒ Failed to crawl pages: {e}")
            # å³ä½¿çˆ¬å–å¤±è´¥ï¼Œä¹Ÿå°è¯•è¿”å›ç©ºDataFrameè€Œä¸æ˜¯æŠ¥é”™
            logger.info("âš ï¸ Returning empty DataFrame due to crawl failure")
            return pd.DataFrame(columns=['title', 'content', 'pub_time', 'url'])
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if not items:
            logger.warning("âš ï¸ No items collected from crawling")
            return pd.DataFrame(columns=['title', 'content', 'pub_time', 'url'])
        
        logger.info(f"ğŸ“Š Processing {len(items)} collected items...")
        
        df = pd.DataFrame(items)
        
        # å¤„ç†æ—¶é—´å­—æ®µ
        if not df.empty and 'publish_time' in df.columns:
            df['publish_time'] = pd.to_datetime(df['publish_time'], errors='coerce')
            end_dt = pd.to_datetime(trigger_time, errors='coerce')
            mask = pd.Series(True, index=df.index)
            if not pd.isna(end_dt):
                start_dt = end_dt - pd.Timedelta(days=1)
                mask &= (df['publish_time'] >= start_dt) & (df['publish_time'] < end_dt)
            df = df.loc[mask].reset_index(drop=True)
            df['pub_time'] = df['publish_time'].dt.strftime("%Y-%m-%d %H:%M:%S")
        else:
            df['pub_time'] = ''

        if 'intro' in df.columns:
            df['content'] = df['intro'].apply(lambda x: self._clean_whitespace(self._strip_html_tags(str(x))))
        else:
            df['content'] = ""

        df['pub_time'] = df['pub_time'].fillna('')

        # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„åˆ—éƒ½å­˜åœ¨
        keep_cols = ['title', 'content', 'pub_time', 'url']
        for col in keep_cols:
            if col not in df.columns:
                df[col] = ""

        df = df[keep_cols].copy()
        
        # ä½¿ç”¨LLMæ•´ç†æ–°é—»å†…å®¹
        if not df.empty:
            df = await self.process_news_with_llm(df)
        
        logger.info(f"get sina news until {trigger_time} success. Total {len(df)} rows")
        return df

if __name__ == "__main__":
    crawler = SinaNewsCrawl(start_page=1, end_page=50)
    df = asyncio.run(crawler.get_data("2025-08-21 15:00:00"))
    print(len(df))
    # try:
    #     output_path = os.path.join(os.path.dirname(__file__), "sina_news_crawl.json")
    #     df.to_json(output_path, orient="records", force_ascii=False, date_format="iso")
    #     print(f"Saved JSON to: {output_path}")
    # except Exception as e:
    #     print(f"Failed to save JSON: {e}")
 