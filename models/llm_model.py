"""
OpenAIæ¨¡å‹å®ç°

æ­¤æ¨¡å—ä¸ºOpenAIæ¨¡å‹æä¾›BaseAgentModelçš„å®ç°ã€‚
é‡‡ç”¨å¼‚æ­¥ä¼˜å…ˆçš„æ–¹æ³•ï¼Œä¸»è¦å®ç°æ˜¯å¼‚æ­¥æµå¼æ–¹æ³•(a_stream_run)ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- æ”¯æŒOpenAI APIçš„åŒæ­¥å’Œå¼‚æ­¥è°ƒç”¨
- å®ç°æµå¼å’Œéæµå¼å“åº”å¤„ç†
- æä¾›é‡è¯•æœºåˆ¶å’Œé”™è¯¯å¤„ç†
- æ”¯æŒæ¨ç†æ¨¡å‹(reasoner)çš„ç‰¹æ®Šå¤„ç†
- è‡ªåŠ¨é…ç½®ç®¡ç†å’Œå®¢æˆ·ç«¯åˆå§‹åŒ–
"""

import os
import sys
import httpx
import openai
import asyncio
from pathlib import Path
from openai import OpenAI, AsyncOpenAI
from openai.types.chat import ChatCompletionChunk
from typing import Dict, List, Optional, AsyncIterator, Callable
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from config.config import cfg
from .base_agent_model import (
    BaseAgentModel,
    AsyncResponseStream,
    StreamingChunk,
    ModelResponse
)

class LLMModelConfig:
    """
    LLMæ¨¡å‹é…ç½®ç±»
    
    ç”¨äºå­˜å‚¨å’Œç®¡ç†LLMæ¨¡å‹çš„é…ç½®å‚æ•°ï¼ŒåŒ…æ‹¬APIå¯†é’¥ã€åŸºç¡€URLã€
    é‡è¯•è®¾ç½®ã€è¶…æ—¶è®¾ç½®ç­‰ã€‚
    """
    
    def __init__(self, model_name: str, api_key: str, base_url: str,
                 max_retries: int = 3, retry_delay: float = 20.0, timeout: float = 60.0, extra_headers: dict = None, proxys: dict = None):
        """
        åˆå§‹åŒ–LLMæ¨¡å‹é…ç½®
        
        å‚æ•°:
            model_name (str): æ¨¡å‹åç§°ï¼Œå¦‚"deepseek-chat"
            api_key (str): APIå¯†é’¥
            base_url (str): APIåŸºç¡€URL
            max_retries (int, å¯é€‰): æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä¸º3
            retry_delay (float, å¯é€‰): é‡è¯•å»¶è¿Ÿæ—¶é—´(ç§’)ï¼Œé»˜è®¤ä¸º20.0
            timeout (float, å¯é€‰): è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)ï¼Œé»˜è®¤ä¸º60.0
            extra_headers (dict, å¯é€‰): é¢å¤–çš„HTTPå¤´ï¼Œé»˜è®¤ä¸ºNone
            proxys (dict, å¯é€‰): ä»£ç†è®¾ç½®ï¼Œé»˜è®¤ä¸ºNone
        """
        self.model_name = model_name    # æ¨¡å‹åç§°
        self.api_key = api_key          # APIå¯†é’¥
        self.base_url = base_url        # APIåŸºç¡€URL
        self.max_retries = max_retries  # æœ€å¤§é‡è¯•æ¬¡æ•°
        self.retry_delay = retry_delay  # é‡è¯•å»¶è¿Ÿæ—¶é—´
        self.timeout = timeout          # è¯·æ±‚è¶…æ—¶æ—¶é—´
        self.extra_headers = extra_headers  # é¢å¤–HTTPå¤´
        self.proxys = proxys            # ä»£ç†è®¾ç½®


class LLMModel(BaseAgentModel):
    """
    OpenAIæ¨¡å‹å®ç°ç±»
    
    æ­¤ç±»æä¾›BaseAgentModelçš„å…·ä½“å®ç°ã€‚
    éµå¾ªå¼‚æ­¥ä¼˜å…ˆçš„æ–¹æ³•ï¼Œä¸»è¦å®ç°a_stream_runæ–¹æ³•ï¼Œ
    è€Œæ‰€æœ‰å…¶ä»–æ–¹æ³•(run, a_run, stream_run)ç”±åŸºç±»å¤„ç†ã€‚
    
    ä¸»è¦ç‰¹æ€§ï¼š
    - æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥OpenAIå®¢æˆ·ç«¯
    - è‡ªåŠ¨é‡è¯•æœºåˆ¶å’Œé”™è¯¯å¤„ç†
    - æ”¯æŒä»£ç†å’Œè‡ªå®šä¹‰HTTPå¤´
    - å¤„ç†æ¨ç†æ¨¡å‹çš„ç‰¹æ®Šå‚æ•°
    """
    
    def __init__(
        self,
        config: LLMModelConfig,
        **kwargs
    ):
        """
        åˆå§‹åŒ–OpenAIæ¨¡å‹
        
        å‚æ•°:
            config (LLMModelConfig): LLMæ¨¡å‹é…ç½®å¯¹è±¡
            **kwargs: é¢å¤–çš„é…ç½®å‚æ•°
        """
        super().__init__(config, **kwargs)
        
        # ä¿å­˜é…ç½®ä¿¡æ¯
        self.config = config
        self.model_name = config.model_name
        self.api_key = config.api_key
        self.base_url = config.base_url
        self.extra_headers = config.extra_headers
        self.proxys = config.proxys
        
        # å¦‚æœAPIå¯†é’¥æˆ–åŸºç¡€URLæœªæä¾›ï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
        if self.api_key is None:
            self.api_key = os.environ.get("OPENAI_API_KEY")
        if self.base_url is None:
            self.base_url = os.environ.get("OPENAI_BASE_URL")

        # åˆå§‹åŒ–åŒæ­¥å®¢æˆ·ç«¯
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            http_client=httpx.Client(proxy=self.proxys) if self.proxys else None
        )
        
        # åˆå§‹åŒ–å¼‚æ­¥å®¢æˆ·ç«¯
        self.async_client = AsyncOpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            http_client=httpx.AsyncClient(proxy=self.proxys) if self.proxys else None
        )

        # å¦‚æœæä¾›äº†é¢å¤–HTTPå¤´ï¼Œåº”ç”¨åˆ°å®¢æˆ·ç«¯
        if self.extra_headers is not None:
            self.client = self.client.with_options(
                default_headers=self.extra_headers)
            self.async_client = self.async_client.with_options(
                default_headers=self.extra_headers)
    
    def _process_chunk(self, chunk: ChatCompletionChunk) -> StreamingChunk[str]:
        """
        å¤„ç†æ¥è‡ªOpenAIçš„æµå¼æ•°æ®å—
        
        ä»OpenAIçš„ChatCompletionChunkä¸­æå–å†…å®¹ï¼Œå¹¶è½¬æ¢ä¸ºæ ‡å‡†çš„StreamingChunkæ ¼å¼ã€‚
        ç‰¹åˆ«å¤„ç†æ¨ç†æ¨¡å‹(reasoner)çš„reasoning_contentå­—æ®µã€‚
        
        å‚æ•°:
            chunk (ChatCompletionChunk): OpenAIè¿”å›çš„æ•°æ®å—
            
        è¿”å›:
            StreamingChunk[str]: æ ‡å‡†åŒ–çš„æµå¼æ•°æ®å—å¯¹è±¡
        """
        # ä»æ•°æ®å—ä¸­æå–å†…å®¹
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¨ç†å†…å®¹ï¼ˆæ¨ç†æ¨¡å‹ç‰¹æœ‰ï¼‰
        if hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content:
            content = chunk.choices[0].delta.reasoning_content  # æ¨ç†è¿‡ç¨‹å†…å®¹
            is_reasoning = True
        else:
            content = chunk.choices[0].delta.content or ""  # æ™®é€šå†…å®¹
            is_reasoning = False
            
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€åä¸€ä¸ªæ•°æ®å—
        is_finished = len(chunk.choices) > 0 and chunk.choices[0].finish_reason is not None
        
        return StreamingChunk(
            content=content,          # æ•°æ®å—å†…å®¹
            is_finished=is_finished,  # æ˜¯å¦ç»“æŸæ ‡å¿—
            raw_chunk=chunk,         # åŸå§‹æ•°æ®å—
            is_reasoning=is_reasoning # æ˜¯å¦ä¸ºæ¨ç†å†…å®¹
        )

    async def a_run_with_semaphore(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        max_retries: Optional[int] = None,
        retry_delay: Optional[float] = None,
        timeout: Optional[float] = None,
        semaphore: Optional[asyncio.Semaphore] = None,
        **kwargs
    ) -> ModelResponse[str]:
        """
        ä½¿ç”¨ä¿¡å·é‡å¼‚æ­¥è¿è¡Œæ¨¡å‹å¹¶æµå¼è¿”å›å“åº”
        
        è¿™æ˜¯æ‰€æœ‰å…¶ä»–æ–¹æ³•(run, a_run, stream_run)å°†ä½¿ç”¨çš„åŸºç¡€å®ç°ã€‚
        ä½¿ç”¨ä¿¡å·é‡æ¥æ§åˆ¶å¹¶å‘è¯·æ±‚æ•°é‡ï¼Œé¿å…è¿‡å¤šå¹¶å‘è¯·æ±‚å¯¼è‡´APIé™åˆ¶ã€‚
        
        å‚æ•°:
            messages (List[Dict[str, str]]): æ¶ˆæ¯å­—å…¸åˆ—è¡¨
            temperature (float, å¯é€‰): é‡‡æ ·æ¸©åº¦ï¼Œé»˜è®¤ä¸º0.7
            max_tokens (Optional[int], å¯é€‰): æœ€å¤§tokenæ•°é‡
            max_retries (Optional[int], å¯é€‰): æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay (Optional[float], å¯é€‰): é‡è¯•å»¶è¿Ÿæ—¶é—´
            timeout (Optional[float], å¯é€‰): è¶…æ—¶æ—¶é—´
            semaphore (Optional[asyncio.Semaphore], å¯é€‰): å¹¶å‘æ§åˆ¶ä¿¡å·é‡
            **kwargs: é¢å¤–çš„æ¨¡å‹ç‰¹å®šå‚æ•°
            
        è¿”å›:
            ModelResponse[str]: æ¨¡å‹å“åº”å¯¹è±¡ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        async with semaphore:
            try:
                response = await self.a_run(
                    messages, 
                    temperature=temperature, 
                    max_tokens=max_tokens, 
                    max_retries=max_retries, 
                    retry_delay=retry_delay, 
                    timeout=timeout, 
                    **kwargs
                )
                return response
            except Exception as e:
                print(f"Error: {e}")
                return None


    async def a_run(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        verbose: bool = False,
        max_retries: Optional[int] = None,
        retry_delay: Optional[float] = None,
        timeout: Optional[float] = None,
        post_process_func: Optional[Callable[[str], str]] = None,
        **kwargs
    ) -> ModelResponse[str]:
        """
        å¼‚æ­¥è¿è¡Œæ¨¡å‹å¹¶è¿”å›å®Œæ•´å“åº”
        
        è¿™æ˜¯a_stream_run()çš„åŒ…è£…å™¨ï¼Œæ”¶é›†æ‰€æœ‰æ•°æ®å—å¹¶ç»„åˆæˆå•ä¸ªå“åº”ã€‚
        å­ç±»åº”è¯¥å®ç°a_stream_run()ä½œä¸ºä¸»è¦æ–¹æ³•ï¼Œæ­¤æ–¹æ³•å°†è‡ªåŠ¨å¤„ç†ã€‚
        
        å‚æ•°:
            messages (List[Dict[str, str]]): æ¶ˆæ¯å­—å…¸åˆ—è¡¨ï¼ŒåŒ…å«'role'å’Œ'content'é”®
            temperature (float, å¯é€‰): é‡‡æ ·æ¸©åº¦(0.0åˆ°1.0)ï¼Œé»˜è®¤ä¸º0.7
            max_tokens (Optional[int], å¯é€‰): ç”Ÿæˆçš„æœ€å¤§tokenæ•°é‡ï¼Œé»˜è®¤ä¸ºNone
            verbose (bool, å¯é€‰): æ˜¯å¦æ‰“å°è¯¦ç»†è¾“å‡ºï¼Œé»˜è®¤ä¸ºFalse
            max_retries (Optional[int], å¯é€‰): æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä¸ºNone
            retry_delay (Optional[float], å¯é€‰): é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼Œé»˜è®¤ä¸ºNone
            timeout (Optional[float], å¯é€‰): è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤ä¸ºNone
            post_process_func (Optional[Callable[[str], str]], å¯é€‰): åå¤„ç†å‡½æ•°ï¼Œé»˜è®¤ä¸ºNone
            **kwargs: é¢å¤–çš„æ¨¡å‹ç‰¹å®šå‚æ•°
            
        è¿”å›:
            ModelResponse[str]: åŒ…å«ç”Ÿæˆå†…å®¹çš„æ¨¡å‹å“åº”å¯¹è±¡
        """

        # è®¾ç½®é»˜è®¤å€¼
        if max_retries is None:
            max_retries = getattr(self, 'config', LLMModelConfig("", "", "")).max_retries
        if retry_delay is None:
            retry_delay = getattr(self, 'config', LLMModelConfig("", "", "")).retry_delay
        if timeout is None:
            timeout = 60

        # é‡è¯•æœºåˆ¶
        for attempt in range(max_retries + 1):
            try:
                # è·å–æµå¼å“åº”
                stream = await self.a_stream_run(
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    timeout=timeout,
                    **kwargs
                )
                
                # æ”¶é›†æ‰€æœ‰æ•°æ®å—
                reasoning_content = ""  # æ¨ç†è¿‡ç¨‹å†…å®¹
                full_content = ""      # ä¸»è¦å†…å®¹
                raw_chunks = []        # åŸå§‹æ•°æ®å—åˆ—è¡¨
                
                # å¼‚æ­¥è¿­ä»£æµå¼å“åº”
                async for chunk in stream:
                    if chunk.is_reasoning:
                        reasoning_content += chunk.content  # æ·»åŠ æ¨ç†å†…å®¹
                    else:
                        full_content += chunk.content      # æ·»åŠ ä¸»è¦å†…å®¹
                    
                    # ä¿å­˜åŸå§‹æ•°æ®å—
                    if chunk.raw_chunk is not None:
                        raw_chunks.append(chunk.raw_chunk)
                        
                        # å¦‚æœå¯ç”¨è¯¦ç»†æ¨¡å¼ï¼Œå®æ—¶æ‰“å°å†…å®¹
                        if verbose:
                            print(chunk.content, end="", flush=True)
            
                # åº”ç”¨åå¤„ç†å‡½æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
                if post_process_func is not None:
                    proc_response = post_process_func(full_content)
                else:
                    proc_response = None

                # åˆ›å»ºåŒ…å«æ”¶é›†å†…å®¹çš„å“åº”å¯¹è±¡
                return ModelResponse(
                    content=self.postprocess_response(full_content),  # åå¤„ç†ä¸»è¦å†…å®¹
                    reasoning_content=reasoning_content,              # æ¨ç†å†…å®¹
                    model_name=self.model_name,                       # æ¨¡å‹åç§°
                    raw_response=raw_chunks if raw_chunks else None,  # åŸå§‹å“åº”æ•°æ®
                    proc_response=proc_response                      # åå¤„ç†å“åº”
                )
            except Exception as e:
                if attempt < max_retries:
                    print(f"ğŸ”„ LLM APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries + 1}): {type(e).__name__}: {e}")
                    print(f"â³ ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                    await asyncio.sleep(retry_delay)
                    continue
                else:
                    print(f"âŒ LLM APIè°ƒç”¨æœ€ç»ˆå¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {type(e).__name__}: {e}")
                    raise
    

    async def a_stream_run(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        max_retries: Optional[int] = None,
        retry_delay: Optional[float] = None,
        timeout: Optional[float] = None,
        **kwargs
    ) -> AsyncResponseStream[str]:
        """
        å¼‚æ­¥è¿è¡Œæ¨¡å‹å¹¶æµå¼è¿”å›å“åº”ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        
        è¿™æ˜¯æ‰€æœ‰å…¶ä»–æ–¹æ³•(run, a_run, stream_run)å°†ä½¿ç”¨çš„ä¸»è¦å®ç°ã€‚
        æä¾›è‡ªåŠ¨é‡è¯•æœºåˆ¶å’Œè¶…æ—¶å¤„ç†ï¼Œç¡®ä¿APIè°ƒç”¨çš„å¯é æ€§ã€‚
        
        å‚æ•°:
            messages (List[Dict[str, str]]): æ¶ˆæ¯å­—å…¸åˆ—è¡¨ï¼ŒåŒ…å«'role'å’Œ'content'é”®
            temperature (float, å¯é€‰): é‡‡æ ·æ¸©åº¦(0.0åˆ°1.0)ï¼Œé»˜è®¤ä¸º0.7
            max_tokens (Optional[int], å¯é€‰): ç”Ÿæˆçš„æœ€å¤§tokenæ•°é‡ï¼Œé»˜è®¤ä¸ºNone
            max_retries (Optional[int], å¯é€‰): æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä¸ºNoneï¼ˆä½¿ç”¨é…ç½®é»˜è®¤å€¼3ï¼‰
            retry_delay (Optional[float], å¯é€‰): é‡è¯•å»¶è¿Ÿæ—¶é—´(ç§’)ï¼Œé»˜è®¤ä¸ºNoneï¼ˆä½¿ç”¨é…ç½®é»˜è®¤å€¼20.0ï¼‰
            timeout (Optional[float], å¯é€‰): æ¯æ¬¡å°è¯•çš„è¶…æ—¶æ—¶é—´(ç§’)ï¼Œé»˜è®¤ä¸ºNoneï¼ˆä½¿ç”¨é…ç½®é»˜è®¤å€¼60.0ï¼‰
            **kwargs: é¢å¤–çš„æ¨¡å‹ç‰¹å®šå‚æ•°
            
        è¿”å›:
            AsyncResponseStream[str]: äº§ç”Ÿç”Ÿæˆå†…å®¹æ•°æ®å—çš„å¼‚æ­¥å“åº”æµ
            
        å¼‚å¸¸:
            - é‡è¯•æ¬¡æ•°ç”¨å®ŒåæŠ›å‡ºæœ€åä¸€æ¬¡çš„å¼‚å¸¸
            - æ”¯æŒè¶…æ—¶ã€è¿æ¥é”™è¯¯ç­‰å¸¸è§APIé”™è¯¯çš„è‡ªåŠ¨é‡è¯•
        """
        
        # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        if max_retries is None:
            max_retries = getattr(self, 'config', LLMModelConfig("", "", "")).max_retries
        if retry_delay is None:
            retry_delay = getattr(self, 'config', LLMModelConfig("", "", "")).retry_delay
        if timeout is None:
            timeout = getattr(self, 'config', LLMModelConfig("", "", "")).timeout
        
        # é‡è¯•å¾ªç¯
        for attempt in range(max_retries + 1):
            try:
                # ä½¿ç”¨asyncio.wait_forè®¾ç½®è¶…æ—¶
                return await asyncio.wait_for(
                    self._internal_a_stream_run(messages, temperature, max_tokens, **kwargs),
                    timeout=timeout
                )
            except (
                asyncio.TimeoutError,        # asyncioè¶…æ—¶
                openai.APITimeoutError,      # OpenAI APIè¶…æ—¶
                openai.APIConnectionError,   # OpenAI APIè¿æ¥é”™è¯¯
                ConnectionError,             # é€šç”¨è¿æ¥é”™è¯¯
                TimeoutError                 # é€šç”¨è¶…æ—¶é”™è¯¯
            ) as e:
                if attempt < max_retries:
                    print(f"ğŸ”„ LLM APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries + 1}): {type(e).__name__}: {e}")
                    print(f"â³ ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                    await asyncio.sleep(retry_delay)  # ç­‰å¾…åé‡è¯•
                    continue
                else:
                    print(f"âŒ LLM APIè°ƒç”¨æœ€ç»ˆå¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {type(e).__name__}: {e}")
                    raise  # é‡è¯•æ¬¡æ•°ç”¨å®Œï¼ŒæŠ›å‡ºå¼‚å¸¸


    async def _internal_a_stream_run(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> AsyncResponseStream[str]:
        """
        å†…éƒ¨å¼‚æ­¥æµå¼è¿è¡Œå®ç°ï¼ˆæ— é‡è¯•é€»è¾‘ï¼‰
        
        è¿™æ˜¯å®é™…çš„APIè°ƒç”¨å®ç°ï¼Œä¸åŒ…å«é‡è¯•é€»è¾‘ã€‚
        è´Ÿè´£å¤„ç†æ¶ˆæ¯é¢„å¤„ç†ã€å‚æ•°å‡†å¤‡ã€APIè°ƒç”¨å’Œå“åº”æµå¤„ç†ã€‚
        
        å‚æ•°:
            messages (List[Dict[str, str]]): æ¶ˆæ¯å­—å…¸åˆ—è¡¨ï¼ŒåŒ…å«'role'å’Œ'content'é”®
            temperature (float, å¯é€‰): é‡‡æ ·æ¸©åº¦(0.0åˆ°1.0)ï¼Œé»˜è®¤ä¸º0.7
            max_tokens (Optional[int], å¯é€‰): ç”Ÿæˆçš„æœ€å¤§tokenæ•°é‡ï¼Œé»˜è®¤ä¸ºNone
            **kwargs: é¢å¤–çš„æ¨¡å‹ç‰¹å®šå‚æ•°
            
        è¿”å›:
            AsyncResponseStream[str]: äº§ç”Ÿç”Ÿæˆå†…å®¹æ•°æ®å—çš„å¼‚æ­¥å“åº”æµ
        """
        # é¢„å¤„ç†æ¶ˆæ¯
        processed_messages = self.preprocess_messages(messages)
        
        # å‡†å¤‡APIè°ƒç”¨å‚æ•°
        params = {
            "model": self.model_name,        # æ¨¡å‹åç§°
            "messages": processed_messages,  # å¤„ç†åçš„æ¶ˆæ¯
            "temperature": temperature,      # é‡‡æ ·æ¸©åº¦
            "stream": True,                 # å¯ç”¨æµå¼å“åº”
            **kwargs                        # å…¶ä»–å‚æ•°
        }
        
        # å¦‚æœæŒ‡å®šäº†æœ€å¤§tokenæ•°é‡ï¼Œæ·»åŠ åˆ°å‚æ•°ä¸­
        if max_tokens is not None:
            params["max_tokens"] = max_tokens
        
        # å¤„ç†æ¨ç†æ¨¡å‹ç‰¹æ®Šå‚æ•°
        if 'thinking' in params:
            thinking_flag = params.pop('thinking')  # ç§»é™¤thinkingå‚æ•°
            if thinking_flag:
                params['extra_body'] = {"thinking": {"type": "enabled"}}   # å¯ç”¨æ¨ç†
            else:
                params['extra_body'] = {"thinking": {"type": "disabled"}}  # ç¦ç”¨æ¨ç†

        # è°ƒç”¨OpenAI API
        stream = await self.async_client.chat.completions.create(**params)
        
        # åˆ›å»ºå¼‚æ­¥è¿­ä»£å™¨æ¥å¤„ç†æ•°æ®å—
        async def chunk_iterator() -> AsyncIterator[StreamingChunk[str]]:
            """å¼‚æ­¥æ•°æ®å—è¿­ä»£å™¨"""
            async for chunk in stream:
                if not chunk.choices:  # è·³è¿‡ç©ºçš„æ•°æ®å—
                    continue
                yield self._process_chunk(chunk)  # å¤„ç†å¹¶äº§ç”Ÿæ•°æ®å—
        
        return AsyncResponseStream(
            iterator=chunk_iterator(),
            model_name=self.model_name
        )

# å…¨å±€LLMé…ç½®å’Œå®ä¾‹
# =======================

# åˆ›å»ºæ™®é€šæ¨¡å¼çš„å…¨å±€LLMé…ç½®
GLOBAL_LLM_CONFIG = LLMModelConfig(
    model_name=cfg.llm["model_name"],    # ä»é…ç½®æ–‡ä»¶è¯»å–æ¨¡å‹åç§°
    api_key=cfg.llm["api_key"],          # ä»é…ç½®æ–‡ä»¶è¯»å–APIå¯†é’¥
    base_url=cfg.llm["base_url"]         # ä»é…ç½®æ–‡ä»¶è¯»å–åŸºç¡€URL
)

# åˆ›å»ºæ™®é€šæ¨¡å¼çš„å…¨å±€LLMå®ä¾‹
GLOBAL_LLM = LLMModel(GLOBAL_LLM_CONFIG)

# å°è¯•åˆ›å»ºæ€è€ƒæ¨¡å¼çš„å…¨å±€LLMé…ç½®å’Œå®ä¾‹
try:
    GLOBAL_THINKING_LLM_CONFIG = LLMModelConfig(
        model_name=cfg.llm_thinking["model_name"],    # ä»é…ç½®æ–‡ä»¶è¯»å–æ€è€ƒæ¨¡å‹åç§°
        api_key=cfg.llm_thinking["api_key"],          # ä»é…ç½®æ–‡ä»¶è¯»å–æ€è€ƒæ¨¡å‹APIå¯†é’¥
        base_url=cfg.llm_thinking["base_url"]         # ä»é…ç½®æ–‡ä»¶è¯»å–æ€è€ƒæ¨¡å‹åŸºç¡€URL
    )
    GLOBAL_THINKING_LLM = LLMModel(GLOBAL_THINKING_LLM_CONFIG)
except Exception as e:
    print(f"åŠ è½½thinkingæ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨llmæ¨¡å‹æ›¿ä»£: {e}")
    # å¦‚æœæ€è€ƒæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šæ¨¡å‹ä½œä¸ºæ›¿ä»£
    GLOBAL_THINKING_LLM = GLOBAL_LLM

# VLMæ¨¡å‹å·²ç§»é™¤ï¼Œvisionèƒ½åŠ›ä¸å¯ç”¨
GLOBAL_VISION_LLM = None

# æ¨¡å—æµ‹è¯•å…¥å£
if __name__ == "__main__":
    pass
